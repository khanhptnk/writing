\documentclass[11pt,letterpaper]{article}
\usepackage[top=1in,bottom=1in,left=1in,right=1in]{geometry}
\usepackage[numbers]{natbib}      % http://merkel.zoneo.net/Latex/natbib.php
\usepackage{palatino}
\bibpunct{(}{)}{;}{a}{,}{,}
\usepackage{chngpage}
\usepackage{stmaryrd}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{lscape}
\usepackage{subfigure}
\usepackage{parskip}
\usepackage[usenames,dvipsnames]{color}
\usepackage{indentfirst}
\definecolor{myblue}{rgb}{0,0.1,0.6}
\definecolor{mygreen}{rgb}{0,0.3,0.1}
\usepackage[colorlinks=true,linkcolor=black,citecolor=mygreen,urlcolor=myblue]{hyperref}

\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand{\ignore}[1]{}

\bibliographystyle{plainnat}
\setlength{\parindent}{30pt}
\linespread{1}

\title{
	Regularization\footnote{This material is for self-study only.}
}

\author{
	Khanh Nguyen
}

\date{June 14, 2015}

\begin{document}
\maketitle

\section{Overview}

Overfitting is a persistent problem in machine learning. Although, in most cases, training machine learning models requires maximizing the likelihood of training data, the ultimate goal is about making the model be able to generalize its predictive capability to unseen data. Therefore, if the training objective function consists of only the likelihood of the training dataset, the model will perform poorly on an unseen dataset if its distribution is different from the training set. This phenomenon is known as \emph{overfitting}. From a different perspective, overfitting is the case when the model's variance is high, i.e. it alters dramatically on different datasets. 

To avoid overfitting, a model should be given information beyond its training dataset. Hence, the general purpose of regularization methods is to inject \emph{inductive bias} into the model. For example,
\begin{itemize}
\item Prevent the training algorithm from picking undesirable parameters (e.g. all-zero or overflowed parameters). 
\item Direct the training algorithm to picking desirable parameters (e.g. sparse parameters).  
\item Represent the prior(s) of the data generating process. This prior acts as a list of preference scores over all parameter configurations. This is the generalization of the two points above.
\end{itemize}

Regularization can be achieved by several ways. One of the most popular forms of regularization is called \emph{regularization penalty}.  In this case, an extra term that is dependent on the model parameters  is added to the training objective. This term is then trained jointly with the log likelihood of the training data. There is also a regularization strength constant to control the degree of regularization. Regularization penalty is successfully used in many classic models such as logistic regression or conditional random fields. However, there are two major drawbacks with this approach: (a) non-smooth regularization terms might hinder optimization; (b) tuning the regularization strength requires training the model many times. Hence, it is not very suitable for non-convex and complex models such as large-scaled neural networks. Modern approach on regularization for these types of models involves techniques that do not modify the training objective. Two most common ones are \emph{early stopping} and \emph{dropout}. At first, they are not intuitively recognized as regularization techniques. However, their effects on the controlling model parameters has been shown. For example, one can show that early stopping in linear regression is equivalent to L2-regularization. Moreover, they are easy to implement but still efficient in practice. 

In this note, I will discuss all of those regularization methods mentioned so far. This material is largely based on the book \emph{Deep Learning} by Yoshua Bengio, Ian J. Goodfellow and Aaron Courville, and the book \emph{Machine Learning: A Probabilistic Perspective} by Kevin Murphy. 

\section{L2-regularization}

The L2-regularizer, also known as weight decay, penalizes the training objective by a norm-2 of the parameter vector (or the weight vector) $w$:
$$
J(w) = NLL(w) + \frac{1}{2} \alpha \norm{w}_2
$$ where NLL(w) is the negative log-likelihood of the training data and $\norm{w}_2 = \sum_i w_i^2$, and $\alpha$ is the regularization strength \footnote{Notice that to penalize the objective, we have to "add" the regularizer since the goal is to minimize the objective.}.

The gradient of the object with respect to $w$ is:
\begin{equation}
 \nabla J(w) = \nabla_w NLL(w) + \alpha w
\end{equation}

Consider a quadratic approximation of the negative log-likelihood near the neighborhood of the optimal value parameters $w^*$:
$$ \hat{NLL}(w) = NLL(w^*) + \frac{1}{2}(w - w^*)^T H(w - w^*)$$ where $H$ is the Hessian matrix of $NLL(w)$ with respect to $w$ \footnote{The gradient term does not appear since $w^*$ is an optimum.}.

Then, 
\begin{equation}
\nabla \hat{NLL}_w(w) = H(w - w^*)
\end{equation}

Replace $\nabla_w NLL(w)$ in (1) by the approximation in (2) and set the gradient to be zero, we have:
$$ H(w - w^*) + \alpha w = 0$$
or
$$
w = (H + \alpha I)^{-1}Hw^*
$$

We observe the followings from the above equation:
\begin{itemize}
\item As $\lambda$ goes to, $w$ approaches its optimal value $w^*$ as when training with the log-likelihood only. 

\item Since $H$ is symmetric, $H = Q \Lambda Q^T$, for some orthonormal matrix $Q$ and some diagonal matrix $\Lambda$. Then:
$$ w = Q(\Lambda + \alpha I)^{-1} \Lambda Q^T w^*$$
We interpret the left hand side as rotating $w*$ to the space spanned by the eigenvectors of $H$, scaling each component $i$ by a factor of $\frac{\lambda_i}{\lambda _i + \alpha}$, then rotating it back to the original space.

This process explains the name \emph{weight decay}. Each component of the parameters is suppressed. The strength of suppression varies depending on the magnitude of each component. Particularly, when $\lambda_i$ is negligible, the $\alpha$ term in the denominator will dominate and scale the  corresponding component to almost zero. Hence, the model will be more likely to go with a set of small-valued parameters. As a result, the variance of the parameters is limited to a smaller range so overfitting will be mitigated.



\end{itemize}




 



 
\end{document}






